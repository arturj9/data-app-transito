# -*- coding: utf-8 -*-
"""Projeto Final - Equipe 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mVcx92U1Bo0DDLEX15Pl2F03wCsT1zLI

#Projeto Final

###Importar bibliotecas
"""

# Importação de bibliotecas essenciais
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from scipy import stats

# Configuração do pandas para exibir todas as colunas do dataset
pd.set_option('display.max_columns', None)

# Modelos e Métricas de Avaliação
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import (
    pairwise_distances, classification_report, confusion_matrix,
    accuracy_score, mean_absolute_error, mean_squared_error, max_error, r2_score
)
from sklearn.model_selection import KFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier  # ou qualquer outro modelo
from sklearn.metrics import accuracy_score

# K-Nearest Neighbors
from sklearn.neighbors import KNeighborsClassifier

from imblearn.over_sampling import ADASYN, SMOTE
import statsmodels.api as sm

# Encoder de categorias
!pip install category_encoders
import category_encoders as ce

#import streamlit as st

"""##Obtain

O dataset a ser analisado é o **Boletim de Acidente de Trânsito**, disponibilizado pelo portal de Dados Abertos da Polícia Rodoviária Federal (PRF) para fins de análise e predição (classificação e/ou regressão). O conjunto de dados pode ser acessado no site: https://www.gov.br/prf/pt-br/acesso-a-informacao/dados-abertos/dados-abertos-da-prf. Para este projeto, será utilizado o **Documento CSV de Acidentes 2023 (Agrupados por ocorrência)**, que contém um total de **67.766 linhas**.

Este dataset contém informações detalhadas sobre acidentes de trânsito registrados pela PRF, com as seguintes variáveis:

* **id**: Identificador numérico único para o acidente.
* **data_inversa**:	Data da ocorrência no formato dd/mm/aaaa.
* **dia_semana**:	Dia da semana da ocorrência (ex.: Segunda, Terça).
* **horário**:	Horário da ocorrência no formato hh:mm.
* **uf**:	Unidade da Federação (ex.: MG, PE, DF).
* **br**:	Identificador numérico da rodovia (BR) onde ocorreu o acidente.
* **km**:	Quilômetro da ocorrência, com valores decimais.
* **municipio**:	Nome do município onde ocorreu o acidente.
* **causa_acidente**:	Causa principal do acidente (excluindo acidentes com "Não" como causa).
* **tipo_acidente**:	Tipo de acidente (ex.: Colisão frontal, Saída de pista).
* **classificação_acidente**:	Gravidade do acidente: Sem Vítimas, Com Vítimas Feridas, Com Vítimas Fatais, ou Ignorado.
* **fase_dia**:	Fase do dia no momento do acidente (ex.: Amanhecer, Pleno dia).
* **sentido_via**:	Sentido da via (Crescente ou Decrescente).
* **condicao_meteorologica**:	Condição meteorológica no momento do acidente (ex.: Céu claro, Chuva).
* **tipo_pista**:	Tipo de pista (ex.: Dupla, Simples, Múltipla).
* **tracado_via**:	Descrição do traçado da via.
* **uso_solo**:	Descrição do uso do solo no local do acidente: Urbano=Sim, Rural=Não.
* **pessoas**:	Total de pessoas envolvidas na ocorrência.
* **mortos**:	Total de mortos na ocorrência.
* **feridos_leves**:	Total de pessoas com ferimentos leves.
* **feridos_graves**:	Total de pessoas com ferimentos graves.
* **ilesos**:	Total de pessoas ilesas.
* **ignorados**:	Pessoas cujo estado físico é desconhecido.
* **feridos**:	Total de feridos (soma de feridos leves e graves).
* **veiculos**:	Total de veículos envolvidos no acidente.
* **latitude**:	Latitude do local do acidente (formato geodésico decimal).
* **longitude**:	Longitude do local do acidente (formato geodésico decimal).
* **regional**:	Superintendência regional da PRF responsável pela área do acidente.
* **delegacia**:	Delegacia da PRF correspondente à circunscrição do acidente.
* **uop**:	Unidade Operacional da PRF responsável pela área do acidente.

###Carregar DataSet
"""

from google.colab import drive
drive.mount('/content/drive')

#df = pd.read_csv('/content/drive/MyDrive/TrabalhoCienciaDeDados/datatran2023.csv', encoding='ISO-8859-1', delimiter=';')

df = pd.read_csv('/content/drive/MyDrive/datatran2023.csv', encoding='ISO-8859-1', delimiter=';')

"""##Scrub

A primeira etapa do processo de limpeza é
 entender a estrutura do dataset, por isso, vamos
 inspecionar as primeiras linhas para verificar a
 disposição dos dados e identificar possíveis problemas
 iniciais. Isso nos permite uma visão geral e nos ajuda
 a planejar o processo de limpeza.
"""

# Exibindo as primeiras linhas para analisar a disposição do dataset
df.head()

"""Identificação de Valores Nulos: Uma abordagem eficiente é exibir as colunas que possuem valores ausentes e a quantidade de dados ausentes em cada uma delas. Isso facilita a análise de onde concentrar os esforços de limpeza."""

# Verificar e exibir os valores ausentes, mostrando as linhas e colunas
missing_data = df.isnull()

# Iterar sobre todas as colunas e linhas para mostrar onde estão os valores ausentes
for column in missing_data.columns:
    null_rows = missing_data[missing_data[column] == True].index.tolist()
    if null_rows:
        print(f"Coluna '{column}' tem valores ausentes nas linhas: {null_rows}")

"""Linhas Duplicadas: Nessa etapa iremos verificar se há linhas duplicadas no dataset e, se caso tiver, iremos removê-las e, caso não tiver, irá informar também."""

# Verificar e remover linhas duplicadas
duplicated_rows = df[df.duplicated()]
num_duplicates = len(duplicated_rows)

if num_duplicates > 0:
    print(f"Linhas duplicadas encontradas: {num_duplicates}. Elas serão removidas.")
    df_cleaned = df.drop_duplicates()
else:
    print("Nenhuma linha duplicada encontrada.")

# Observar de maneira geral a quantidade de valores ausentes em cada coluna.
df.isnull().sum()

"""Tratando a coluna 'classificacao_acidente' com valores ausentes: Aqui, utilizamos a moda da classificação de acidente para casos de 'Tombamento', pois é o tipo específico de acidente na linha onde o valor está ausente. Assim, garantimos consistência nos dados ao preencher com a categoria mais comum."""

# Filtrar para 'tombamento'
tombamento_data = df[df['tipo_acidente'] == 'Tombamento']

# Verificar a moda da classificação de acidente para casos de tombamento
classificacao_moda = tombamento_data['classificacao_acidente'].mode()

if not classificacao_moda.empty:
    # Preencher o valor ausente com a moda encontrada
    df['classificacao_acidente'].fillna(classificacao_moda[0], inplace=True)
    print(f"Valor ausente preenchido com: {classificacao_moda[0]}")
else:
    print("Nenhuma moda encontrada para 'Tombamento'.")

# Verificando se a linha [2] da coluna 'tipo_acidente' foi preenchida com sucesso.
# Ou seja devidamente tratada.
df.head()

"""Para tratar dos valores ausentes da coluna 'Regional' este código usa as coordenadas para prever a regional com base nos pontos mais próximos já conhecidos. É uma abordagem baseada em proximidade geográfica e pode melhorar a qualidade dos dados, considerando a relação espacial entre as colunas."""

# Substituir vírgulas por pontos nas colunas de latitude e longitude e converter para float
df['latitude'] = df['latitude'].astype(str).str.replace(',', '.').astype(float)
df['longitude'] = df['longitude'].astype(str).str.replace(',', '.').astype(float)

# Filtrar as linhas onde 'regional', 'latitude' e 'longitude' não são nulos
df_valid = df.dropna(subset=['regional', 'latitude', 'longitude'])

# Filtrar as linhas onde 'regional' está ausente mas 'latitude' e 'longitude' estão presentes
df_missing_regional = df[df['regional'].isnull() & df['latitude'].notnull() & df['longitude'].notnull()]

# Preparar os dados para o modelo KNN
X_train = df_valid[['latitude', 'longitude']]
y_train = df_valid['regional']

# Instanciar e treinar o modelo KNN
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)

# Fazer previsões para as linhas com 'regional' ausente
X_missing = df_missing_regional[['latitude', 'longitude']]
df.loc[df['regional'].isnull(), 'regional'] = knn.predict(X_missing)

# Verificar se ainda restam valores ausentes
print(df['regional'].isnull().sum())

"""Tratando a coluna 'delegacia': Após ser tratada a coluna 'Regional', iremos fazer uma análise comparativa da coluna 'Delegacia' com a coluna 'Regional'. Nesse caso, preenchendo os valores ausentes com a moda de cada regional, ou seja a delegacia mais comum em cada regional."""

# Criar um mapeamento de moda da delegacia para cada regional
delegacia_por_regional = df.groupby('regional')['delegacia'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).to_dict()

# Preencher os valores ausentes na coluna 'delegacia' com base na coluna 'regional'
for index, row in df[df['delegacia'].isnull()].iterrows():
    regional = row['regional']
    if regional in delegacia_por_regional:
        df.at[index, 'delegacia'] = delegacia_por_regional[regional]

# Verificar se ainda há valores ausentes
print(df['delegacia'].isnull().sum())

# Identificar a linha que ainda tem valor ausente
missing_delegacia_row = df[df['delegacia'].isnull()]
print(missing_delegacia_row[['regional', 'latitude', 'longitude']])

regional_faltante = missing_delegacia_row['regional'].values[0]
print(f"Regional com delegacia ausente: {regional_faltante}")
print("Moda de delegacia para essa regional:", delegacia_por_regional.get(regional_faltante))

"""Remoção de Linha com Valor Ausente na Coluna 'Delegacia': Apenas uma linha na coluna 'delegacia' apresenta um valor ausente, associado à regional 'UniPRF'. Após analisar a quantidade de ocorrências dessa regional, verificamos que ela aparecia apenas uma vez, o que inviabiliza a determinação de uma moda ou valor representativo para preenchimento.

Diante disso, optamos por remover a linha correspondente, uma vez que sua ausência não comprometeria a integridade dos dados e garantiria a qualidade da análise subsequente.
"""

# Remover linhas onde 'delegacia' é nula
df = df.dropna(subset=['delegacia'])

# Verificar se ainda há valores ausentes
print(df['delegacia'].isnull().sum())

"""Tratando a coluna 'uop': Utilizando aqui a mesma lógica da coluca 'delegacia', preenchendo os valores ausentes com base nas modas por delegacias, ja que estão diretamente vinculadas."""

# Tratando a coluna 'uop'
# Podemos utilizar a mesma lógica da 'delegacia', preenchendo os valores ausentes
# com base nas modas por regional, ou vinculadas à delegacia correspondente.

# Criar um mapeamento da moda da UOP para cada delegacia
uop_por_delegacia = df.groupby('delegacia')['uop'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).to_dict()

# Preencher os valores ausentes na coluna 'uop' com base na coluna 'delegacia'
for index, row in df[df['uop'].isnull()].iterrows():
    delegacia = row['delegacia']
    if delegacia in uop_por_delegacia:
        df.at[index, 'uop'] = uop_por_delegacia[delegacia]

# Verificar se ainda há valores ausentes
print(df['uop'].isnull().sum())

"""Finalmente, verificando se há ou não ainda a presença de valores nulos nas colunas da base de dados utilzada."""

df.isnull().sum()

"""##Explore
O explore é a etapa inicial da análise de dados onde se busca entender a estrutura, os padrões e as características principais do conjunto de dados,incluindo assim o seu entendimento geral, ou seja a avaliação das colunas, tipos de dados e identificar valores ausentes, de estatísticas descritivas,como o resumo numérico das variáveis, como média, mediana, desvio padrão, etc,a distribuição de dados, podendo verificar a frequência de valores em variáveis categóricas e a distribuição de variáveis numéricas e dentre outros aspectos
"""

#Visualização de estatísticas das variáveis numéricas do dataset
df.describe()

#Visão geral da estrutura do dataset
df.info()

#Visualização das colunas do dataset
df.columns

for col in df.columns:
  print(f'{col}:{df[col].unique()}\n')

"""###Análise geral dos acidentes"""

# Número total de acidentes
total_acidentes = len(df)
print(f'Foram registrados {total_acidentes} acidentes de trânsito no ano de 2023')

"""Proporção por acidentes por condições meteorológicas"""

# Contagem de acidentes por condições meteorológicas
contagem_acidentes_por_condicoes = df['condicao_metereologica'].value_counts()

# Proporção de acidentes por condições meteorológicas
proporcao_acidentes_por_condicoes = df['condicao_metereologica'].value_counts(normalize=True) * 100

print(proporcao_acidentes_por_condicoes)

# Contagem de acidentes por classificação (leve, grave, fatal)
contagem_acidentes_classificacao = df['classificacao_acidente'].value_counts()

# Proporção de acidentes por classificação
proporcao_acidentes_classificacao = df['classificacao_acidente'].value_counts(normalize=True) * 100

print(proporcao_acidentes_classificacao)

# Calcular a contagem de acidentes por causa
contagem_causas = df['causa_acidente'].value_counts()

# Calcular a proporção
proporcao_causas = contagem_causas / contagem_causas.sum() *100

# Exibir o resultado das 10 maiores causas de acidentes em 2024
proporcao_causas.head(10)

"""###Análise da gravidade dos acidentes"""

# Filtrar os acidentes com vítimas fatais
acidentes_fatais = df[df['classificacao_acidente'] == 'Com Vítimas Fatais']

# Agrupar por causa do acidente e contar o número de acidentes fatais por causa
causas_fatais = acidentes_fatais.groupby('causa_acidente').size().reset_index(name='total_acidentes_fatais')

# Ordenar por número de acidentes fatais (do maior para o menor)
causas_fatais = causas_fatais.sort_values(by='total_acidentes_fatais', ascending=False)

# Exibir as causas mais associadas a acidentes fatais
print(causas_fatais)

# Agrupar pela classificação do acidente e calcular a média de veículos envolvidos
gravidade_veiculos = df.groupby('classificacao_acidente')['veiculos'].mean().reset_index()

# Renomear a coluna para refletir que é a média de veículos
gravidade_veiculos.columns = ['classificacao_acidente', 'media_veiculos']

# Ordenar pela gravidade para facilitar a análise
gravidade_veiculos = gravidade_veiculos.sort_values(by='media_veiculos', ascending=False)

#Exibir o resultado
print(gravidade_veiculos)

"""###Analise Exploratória dos Acidentes

#### Função para geração dos gráficos
"""

def plot(xValues, yValues, annotate, number_formatting, title, xLabel, yLabel):
    # Configurações para um estilo mais agradável
    sns.set(style="whitegrid")

    # Configurações da figura e criação do gráfico de barras
    plt.figure(figsize=(10, 6))
    ax = sns.barplot(x=xValues, y=yValues, hue=xValues, palette="viridis", legend=False)

    # Se "annotate" for True, adiciona os valores no topo das barras
    if annotate:
        for p in ax.patches:
            ax.annotate(format(p.get_height(), number_formatting),  # Mantém cinco casas decimais
                        (p.get_x() + p.get_width() / 2., p.get_height()),  # Posição
                        ha='center', va='center', xytext=(0, 5),  # Ajuste de posição do texto
                        textcoords='offset points', fontsize=10)

    # Adiciona o título e os rótulos dos eixos
    plt.title(title, fontsize=16)
    plt.xlabel(xLabel, fontsize=12)
    plt.ylabel(yLabel, fontsize=12)

    # Rotaciona os rótulos do eixo X para melhorar a visualização
    plt.xticks(rotation=45, ha='right')

    # Ajuste do layout para evitar sobreposição
    plt.tight_layout()

    # Exibe o gráfico
    plt.show()

"""#### Número total de acidentes"""

total_acidentes = len(df)

print(f'Foram registrados {total_acidentes} acidentes de trânsito no ano de 2023')

"""#### Qual a distribuição de acidentes por condição meteorológica? (ex.: céu limpo, chuva, neblina)"""

df

# Contagem de acidentes por condições meteorológicas
contagem_acidentes_por_condicoes = df['condicao_metereologica'].value_counts()

# Proporção de acidentes por condições meteorológicas
proporcao_acidentes_por_condicoes = df['condicao_metereologica'].value_counts(normalize=True) * 100

#print(proporcao_acidentes_por_condicoes)

plot(xValues=contagem_acidentes_por_condicoes.index, yValues=proporcao_acidentes_por_condicoes, annotate=True, number_formatting='0.5f', title="Quantidade de Acidentes por Condições Meteorológicas", xLabel="Condições Meteorológicas", yLabel="Quantidade de Acidentes")

"""#### Quais as condições meteorológicas adversas que mais causaram acidentes fatais?"""

# Filtrar dados excluindo "Céu Claro"
df_adversas = df[df['condicao_metereologica'] != 'Céu Claro']

# Filtrar acidentes com vítimas fatais
df_vitimas_fatais = df_adversas[df_adversas['classificacao_acidente'] == 'Com Vítimas Fatais']

# Contar acidentes por condição meteorológica adversa
acidentes_vitimas_fatais = df_vitimas_fatais.groupby('condicao_metereologica').size().sort_values(ascending=False)

print(acidentes_vitimas_fatais)

plot(xValues=acidentes_vitimas_fatais.index, yValues=acidentes_vitimas_fatais, annotate=True, number_formatting='0.1f', title="Condições metereologicas e vítimas fatais", xLabel="Condições Meteorológicas", yLabel="Quantidade de acidentes fatais")

"""#### Qual é a proporção de acidentes leves, graves e fatais no dataset?"""

# Contagem de acidentes por classificação (leve, grave, fatal)
contagem_acidentes_classificacao = df['classificacao_acidente'].value_counts()

# Proporção de acidentes por classificação
proporcao_acidentes_classificacao = df['classificacao_acidente'].value_counts(normalize=True) * 100

#print(proporcao_acidentes_classificacao)

plot(xValues=contagem_acidentes_classificacao.index, yValues=proporcao_acidentes_classificacao, annotate=True, number_formatting='0.5f', title="Proporção de acidentes leves, graves e fatais registrados no dataset", xLabel="Classificação do acidente com relação às vitimas", yLabel="Proporção (em porcentagem)")

"""#### Qual é a proporção por causa no dataset?"""

# Calcular a contagem de acidentes por causa
contagem_causas = df['causa_acidente'].value_counts()

# Calcular a proporção
proporcao_causas = contagem_causas / contagem_causas.sum() *100

plot(xValues=contagem_causas.head(10).index, yValues=proporcao_causas.head(10), annotate=True, number_formatting='0.5f', title="Relação proporcional entre as possíveis causas de acidente", xLabel="Causas de acidentes do dataset", yLabel="Proporção (em porcentagem)")

"""#### Quais causas de acidente estão mais associadas a acidentes fatais?"""

# Filtrar os acidentes com vítimas fatais
acidentes_fatais = df[df['classificacao_acidente'] == 'Com Vítimas Fatais']

# Agrupar por causa do acidente e contar o número de acidentes fatais por causa
causas_fatais = acidentes_fatais.groupby('causa_acidente').size().reset_index(name='total_acidentes_fatais')

# Ordenar por número de acidentes fatais (do maior para o menor)
causas_fatais = causas_fatais.sort_values(by='total_acidentes_fatais', ascending=False)

print(causas_fatais)

plot(xValues=causas_fatais['causa_acidente'].head(15), yValues=causas_fatais['total_acidentes_fatais'].head(15), annotate=True, number_formatting='0.1f', title="Causas de acidentes estão mais associadas a acidentes fatais", xLabel="Causas dos acidentes", yLabel="a")

"""#### Qual a quantidade média de veículos envolvidos e sua relação com a gravidade do acidente?"""

# Agrupar pela classificação do acidente e calcular a média de veículos envolvidos
gravidade_veiculos = df.groupby('classificacao_acidente')['veiculos'].mean().reset_index()

# Renomear a coluna para refletir que é a média de veículos
gravidade_veiculos.columns = ['classificacao_acidente', 'media_veiculos']

# Ordenar pela gravidade para facilitar a análise
gravidade_veiculos = gravidade_veiculos.sort_values(by='media_veiculos', ascending=False)

#print(gravidade_veiculos)

plot(xValues=gravidade_veiculos['classificacao_acidente'], yValues=gravidade_veiculos['media_veiculos'], annotate=True, number_formatting='0.3f', title="Relação entre a Quantidade média de veículos envolvidos e a gravidade do acidente", xLabel="Classificação do acidente com relação às vitimas", yLabel="Quantidade média de veiculos envolvidos")

"""#### Quais os dias da semana que ocorrem maior número de acidentes?"""

# Contar o número de acidentes por dia da semana
acidentes_por_dia = df['dia_semana'].value_counts().reset_index()
acidentes_por_dia.columns = ['dia_semana', 'total_acidentes']

# Ordenar os dias da semana (se necessário, dependendo da ordem original no dataset)
dias_ordenados = ['segunda-feira', 'terça-feira', 'quarta-feira', 'quinta-feira', 'sexta-feira', 'sábado', 'domingo']
acidentes_por_dia['dia_semana'] = pd.Categorical(acidentes_por_dia['dia_semana'], categories=dias_ordenados, ordered=True)
acidentes_por_dia = acidentes_por_dia.sort_values('dia_semana')

print(acidentes_por_dia)

plot(xValues=acidentes_por_dia['dia_semana'], yValues=acidentes_por_dia['total_acidentes'], annotate=True, number_formatting='0.1f', title="Dias da semana em que ocorrem o maior número de acidentes", xLabel="Dias da Semana", yLabel="Quantidade total de acidentes")

"""#### Quais causas de acidentes costumam acontecer em determinado dia da semana?"""

# Contar a frequência de cada causa de acidente em cada dia da semana
causas_dias = df.groupby(['dia_semana', 'causa_acidente']).size().reset_index(name='contagem')

causas_dias = causas_dias.sort_values(by='dia_semana', ascending=True)

causas_dias = causas_dias.sort_values(by='contagem', ascending=False)

print(causas_dias)

dias_da_semana = causas_dias['dia_semana'].unique()

for dia_da_semana in dias_da_semana:
    dia_causas = causas_dias[causas_dias['dia_semana'] == dia_da_semana]
    dia_causas = dia_causas.sort_values(by='contagem', ascending=False).head(10) # O numero dentro da função head() define quantas causas serão mostradas nos graficos gerados
    plot(xValues=dia_causas['causa_acidente'], yValues=dia_causas['contagem'], annotate=True, number_formatting='0.1f', title= f"Causas de acidentes que mais acontecem no(a) {dia_da_semana}", xLabel="Causas dos acidentes", yLabel="Contagem absoluta")

"""#### Em quais regiões ocorrem acidentes mais graves?"""

# Filtrar os dados para acidentes graves (vítimas fatais ou feridas graves)
df['gravidade_acidente'] = df['classificacao_acidente'].apply(
    lambda x: 'Grave' if x in ['Com Vítimas Fatais', 'Com Vítimas Feridas'] else 'Leve')

# Agrupar os dados por regional e gravidade
acidentes_por_regional = df.groupby(['regional', 'gravidade_acidente']).size().reset_index(name='contagem')

# Exibir as 10 regionais com mais acidentes graves
regionais_graves = acidentes_por_regional[acidentes_por_regional['gravidade_acidente'] == 'Grave']
regionais_graves = regionais_graves.sort_values(by='contagem', ascending=False)

#print(regionais_graves)

plot(xValues=regionais_graves['regional'], yValues=regionais_graves['contagem'], annotate=True, number_formatting='0.1f', title="Regiões onde ocorrem os acidentes mais graves", xLabel="", yLabel="")

"""###Etapa de Explore dos Dados
Verificação e tratamento dos dados do dataset para prepara-los para a etapa de modelagem

"""

df_new = df.copy() # cria uma cópia do df original

df_new['data_inversa'] = pd.to_datetime(df_new['data_inversa']) # Formata a data para evitar inconsistências
df_new['horario'] = pd.to_datetime(df_new['horario'], format='%H:%M:%S').dt.time # Formata o horário para evitar inconsistências

df_new['month'] = df_new['data_inversa'].dt.month # Gera uma nova coluna com o dia em que o acidente aconteceu
df_new['day'] = df_new['data_inversa'].dt.day # Gera uma nova coluna com o dia em que o acidente aconteceu
df_new['hour'] = pd.to_datetime(df_new['horario'], format='%H:%M:%S').dt.hour # Gera uma nova coluna com a hora em que o acidente aconteceu

df_new['km'] = df_new['km'].str.replace(',', '.').astype(float) # Formata a coluna 'km' convertendo todos os seus valores para float

# Usa a técnica de Enconding para variáveis categóricas ordinais para transformar as strings object dos dias da semana para números int32
le = LabelEncoder()

colunas_para_label_encode = ['uf', 'causa_acidente', 'tipo_acidente',
    'sentido_via','classificacao_acidente', 'tracado_via', 'condicao_metereologica',
    'tipo_pista', 'regional', "dia_semana", "fase_dia", "br", "uso_solo"]

for coluna in colunas_para_label_encode:
  df_new[coluna] = le.fit_transform(df_new[coluna])

df = df_new

# Aplicação do Frequency Encoding nas colunas de cardinalidade
for col in ['municipio', 'delegacia', 'uop']:
    freq = df[col].value_counts(normalize=True)  # Frequência de cada categoria
    df_new[col + '_freq'] = df_new[col].map(freq)  # Substitui pela frequência

# Aplicação do Binary Encoding nas colunas de cardinalidade
binary_encoder = ce.BinaryEncoder(cols=['classificacao_acidente','municipio', 'delegacia', 'uop', 'condicao_metereologica', 'tipo_pista'])
df_new = binary_encoder.fit_transform(df_new)

df_new.info()

df_new.columns.values

df_new.head()

# Remover colunas do tipo 'object' que não são numéricas
df_numeric = df.select_dtypes(include=['int64', 'float64'])

# Gerar a matriz de correlação
correlation_matrix = df_numeric.corr()

# Exibir a matriz de correlação
#print(correlation_matrix)

# Visualizar a matriz de correlação com um heatmap
plt.figure(figsize=(24, 18))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.1f', linewidths=0.5)
plt.title('Matriz de Correlação')
plt.show()

# Remover colunas do tipo 'object' que não são numéricas
df_numeric = df_new.select_dtypes(include=['int64', 'float64'])

# Gerar a matriz de correlação
correlation_matrix = df_numeric.corr()

# Exibir a matriz de correlação
#print(correlation_matrix)

# Visualizar a matriz de correlação com um heatmap
plt.figure(figsize=(24, 18))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.1f', linewidths=0.5)
plt.title('Matriz de Correlação')
plt.show()

"""####Hipóteses

A partir da matriz de correlação fornecida, é possível interpretar as correlações entre diferentes variáveis do conjunto de dados, ajudando a identificar possíveis padrões e relações que podem ser explorados como hipóteses em análises futuras. Dentre as quais podemos destacar estão:

1 - Acidentes com mais veículos envolvidos tendem a ter mais pessoas afetadas.
Essa correlação sugere que quanto mais veículos estão envolvidos no acidente, maior é o número de pessoas presentes, o que pode aumentar as chances de feridos e vítimas, o que sugere a investigação dos tipos de acidentes e as condições que frequentemente envolvem múltiplos veículos;

2 - A classificação de acidentes pode ser prevista utilizando-se de outras variáveis (colunas).

3 - Correlação entre regiões e a gravidade dos acidentes.
Existem correlação entre certas regiões e a exagerada presença de acidentes e consequentemente mais acidentes graves. Indicando que algumas áreas são mais propensas a presença de acidentes fatais, inferindo que são lugares com pouca infraestrutura e com situações de trânsito perigoso.

##Model

A etapa de Model envolve a seleção, treinamento e avaliação de modelos de aprendizado de máquina para resolver o problema definido nas etapas anteriores

###Hipótese 1
"""

from scipy import stats
F, p = stats.f_oneway(df['veiculos'], df['mortos'])
print('F-statistic:', F)
print('p-value:', p)

X = df[['veiculos',]]
y = df['mortos']

print(y.value_counts())

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = LinearRegression()

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

X_with_constant = sm.add_constant(X)  # Adicionar constante para o intercepto
model = sm.OLS(y, X_with_constant).fit()
print(model.summary())

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R² Score: {r2}')

# Criar o gráfico
plt.figure(figsize=(10, 6))

# Gráfico de dispersão para valores reais
plt.scatter(X_test, y_test, color='blue', label='Valores Reais', s=100)

# Gráfico da linha de regressão
plt.plot(X_test, y_pred, color='red', label='Linha de Regressão', linewidth=2)

# Adicionando título e legendas
plt.title('Regressão Linear: Valores Reais e Linha de Regressão', fontsize=14)
plt.xlabel('Variável Independente', fontsize=12)
plt.ylabel('Variável Dependente', fontsize=12)
plt.legend()
plt.grid(True)

# Mostrar o gráfico
plt.show()

"""###Hipótese 2"""

df.info()

# Selecionar características e alvo
X = df[['causa_acidente', 'tipo_acidente', 'condicao_metereologica', 'veiculos', 'tracado_via', 'dia_semana', 'fase_dia', 'tipo_pista', 'sentido_via', 'uso_solo',
        'pessoas', 'latitude', 'longitude', 'br', 'km']]
y = df['classificacao_acidente']

print(y.value_counts())

smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X, y)

X = X_resampled
y = y_resampled

print(y.value_counts())

# Separar em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Lista de modelos a serem testados
models = {
    'KNeighborsClassifier': KNeighborsClassifier(n_neighbors=3),
    'RandomForestClassifier': RandomForestClassifier(),
    'DecisionTreeClassifier': DecisionTreeClassifier()
}

# Dicionário para armazenar os resultados
results = {}

# Testar cada modelo
for model_name, model in models.items():
    # Criar um pipeline com escalação e o modelo
    pipeline = Pipeline([
        ('scaler', StandardScaler()),  # Escalonamento dos dados
        ('classifier', model)
    ])

    # Treinar o modelo
    pipeline.fit(X_train, y_train)

    # Fazer previsões
    y_pred = pipeline.predict(X_test)

    # Avaliar o modelo
    report = classification_report(y_test, y_pred, output_dict=True)
    results[model_name] = report

# Exibir os resultados
for model_name, report in results.items():
    print(f"Resultados para {model_name}:")
    print(f"Acurácia: {report['accuracy']:.2f}")
    print(f"Precisão: {report['weighted avg']['precision']:.2f}")
    print(f"Recall: {report['weighted avg']['recall']:.2f}")
    print(f"F1-Score: {report['weighted avg']['f1-score']:.2f}\n")

model = RandomForestClassifier()

# Definindo o K-Fold
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Calculando as scores de validação cruzada
scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')

print(f'Scores de Validação Cruzada: {scores}')
print(f'Média da Acurácia: {scores.mean()}')

model.fit(X_train, y_train)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

y_pred = model.predict(X_test)  # Substitua por seu conjunto de teste
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predito')
plt.ylabel('Real')
plt.title('Matriz de Confusão')
plt.show()

importances = model.feature_importances_
indices = np.argsort(importances)[::-1]
features = X.columns  # Substitua se necessário

plt.figure()
plt.title('Importância das Características')
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), features[indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.show()

"""###Hipótese 3"""

# Correlação entre regiões e a gravidade dos acidentes
correlation_matrix = df.groupby('regional')['classificacao_acidente'].value_counts(normalize=True).unstack()
correlation_matrix = correlation_matrix.fillna(0)  # Preencher valores NaN com 0
correlation_matrix = correlation_matrix.corr()

print(correlation_matrix)

# Matriz de correlação em um heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlação entre Regiões e Gravidade dos Acidentes')
plt.xlabel('Classificação do Acidente')
plt.ylabel('Classificação do Acidente')
plt.show()

X = df[['br', 'km', 'latitude', 'longitude', 'regional']]
y = df['classificacao_acidente']

# Dividir os dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

# Fazer previsões no conjunto de teste
y_pred = model.predict(X_test)

# Lista de modelos a serem testados
models = {
    'KNeighborsClassifier': KNeighborsClassifier(n_neighbors=3),
    'RandomForestClassifier': RandomForestClassifier(),
    'DecisionTreeClassifier': DecisionTreeClassifier()
}

# Dicionário para armazenar os resultados
results = {}

# Testar cada modelo
for model_name, model in models.items():
    # Criar um pipeline com escalação e o modelo
    pipeline = Pipeline([
        ('scaler', StandardScaler()),  # Escalonamento dos dados
        ('classifier', model)
    ])

    # Treinar o modelo
    pipeline.fit(X_train, y_train)

    # Fazer previsões
    y_pred = pipeline.predict(X_test)

    # Avaliar o modelo
    report = classification_report(y_test, y_pred, output_dict=True)
    results[model_name] = report

# Exibir os resultados
for model_name, report in results.items():
    print(f"Resultados para {model_name}:")
    print(f"Acurácia: {report['accuracy']:.2f}")
    print(f"Precisão: {report['weighted avg']['precision']:.2f}")
    print(f"Recall: {report['weighted avg']['recall']:.2f}")
    print(f"F1-Score: {report['weighted avg']['f1-score']:.2f}\n")

model = RandomForestClassifier()

# Definindo o K-Fold
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Calculando as scores de validação cruzada
scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')

print(f'Scores de Validação Cruzada: {scores}')
print(f'Média da Acurácia: {scores.mean()}')

model.fit(X_train, y_train)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

y_pred = model.predict(X_test)  # Substitua por seu conjunto de teste
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predito')
plt.ylabel('Real')
plt.title('Matriz de Confusão')
plt.show()

importances = model.feature_importances_
indices = np.argsort(importances)[::-1]
features = X.columns  # Substitua se necessário

plt.figure()
plt.title('Importância das Características')
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), features[indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.show()

"""##Interpret

A etapa de **Interpret** tem como objetivo principal analisar e compreender os resultados obtidos das fases anteriores, transformando os dados em informações úteis e insights. Nessa fase, são feitas interpretações dos padrões e tendências revelados pelos dados, e os resultados são explicados de forma clara e objetiva. A interpretação é essencial para produzir conclusões significativas, permitindo que os dados sejam compreendidos em um contexto real.

### 1. Análise Estatística da Relação entre Veículos Envolvidos e Mortes

Nesta parte, utilizamos a **ANOVA** para testar se o número de veículos envolvidos pode ser um bom preditor do número de mortos em acidentes de trânsito. A **ANOVA** `(Análise de Variância)` nos ajuda a verificar se há uma diferença significativa entre as médias das variáveis analisadas.
"""

from scipy import stats

# Teste ANOVA entre o número de veículos e mortos
F, p = stats.f_oneway(df['veiculos'], df['mortos'])
print('F-statistic:', F)
print('p-value:', p)

"""- **F-statistic:** Os resultados mostraram um valor da estatística F de aproximadamente 156.158, indicando que há uma grande variação entre os grupos (número de veículos e número de mortos) em comparação com a variação dentro dos grupos. Esse valor elevado sugere que a relação entre essas duas variáveis é expressiva.
  - **p-value:** O valor de p obtido foi 0.0, o que significa que a probabilidade de essa relação ocorrer ao acaso é praticamente inexistente. Um valor de p abaixo de 0,05 (neste caso, extremamente abaixo) nos permite concluir que existe uma diferença estatisticamente significativa entre as variáveis, ou seja, o número de veículos envolvidos tem um impacto significativo sobre o número de mortos nos acidentes.

### 2. Regressão Linear entre Veículos e Mortes

A regressão linear foi utilizada para modelar a relação entre o número de veículos envolvidos `(X)` e o número de mortos `(y)`.
"""

from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

# Separar as variáveis independentes (veículos) e dependentes (mortos)
X = df[['veiculos']]
y = df['mortos']

# Divisão dos dados em conjunto de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Ajustar o modelo de regressão linear
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Usando o Statsmodels para obter mais detalhes da regressão
X_with_constant = sm.add_constant(X)
ols_model = sm.OLS(y, X_with_constant).fit()

# Sumário estatístico
print(ols_model.summary())

"""  - **Coeficiente de Regressão (Slope):** O coeficiente para a variável veículos é 0.0382, o que indica que, para cada veículo adicional envolvido no acidente, o número de mortes aumenta em média 0,0382 unidades, mantendo todas as outras variáveis constantes. Isso significa que acidentes envolvendo mais veículos tendem a resultar em um número maior de vítimas fatais.
  - **Intercepto:** O intercepto da regressão é 0.0067, o que representa o valor esperado de mortos quando o número de veículos envolvidos é zero. Embora não tenha um significado prático direto (já que um acidente com zero veículos não ocorre), ele serve como um ponto de referência na equação de regressão.
  - **p-value dos Coeficientes:** O p-value do coeficiente de veículos é 0.000, que está bem abaixo do nível de significância de 0,05. Isso indica que o número de veículos tem um efeito estatisticamente significativo no número de mortos, ou seja, há evidências muito fortes de que o número de veículos envolvidos em um acidente influencia o número de vítimas fatais.
"""

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R² Score: {r2}')

"""- **R² Score:** O valor de R² é 0.020, o que significa que apenas 2% da variação no número de mortos é explicada pelo número de veículos. Embora esse valor seja baixo, ele ainda sugere uma correlação entre as duas variáveis, mas também indica que há muitos outros fatores não incluídos no modelo que podem estar influenciando o número de mortes.

### 3. Visualização Gráfica: Dispersão e Linha de Regressão

A seguir, criamos um gráfico de dispersão dos dados reais de número de veículos e mortos, juntamente com a linha de regressão gerada pelo modelo. Isso ajuda a visualizar se a relação é linear e como o modelo ajusta os dados.
"""

import matplotlib.pyplot as plt

# Criar o gráfico
plt.figure(figsize=(10, 6))

# Gráfico de dispersão para valores reais
plt.scatter(X_test, y_test, color='blue', label='Valores Reais', s=100)

# Gráfico da linha de regressão
plt.plot(X_test, y_pred, color='red', label='Linha de Regressão', linewidth=2)

# Adicionando título e legendas
plt.title('Regressão Linear: Valores Reais e Linha de Regressão', fontsize=14)
plt.xlabel('Número de Veículos', fontsize=12)
plt.ylabel('Número de Mortes', fontsize=12)
plt.legend()
plt.grid(True)

# Mostrar o gráfico
plt.show()

"""- O gráfico de dispersão mostra a relação entre o número de veículos envolvidos no acidente e o número de mortos. Cada ponto azul representa um acidente, enquanto a linha vermelha mostra como o modelo de regressão linear ajusta esses dados.
  - No gráfico em questão, embora alguns pontos estejam próximos da linha, muitos outros estão espalhados e distantes. Isso sugere que o modelo linear não está capturando completamente a relação entre o número de veículos e o número de mortos. A dispersão dos pontos distantes da linha indica que a relação entre essas variáveis pode não ser estritamente linear. É possível que outras variáveis, que não foram incluídas no modelo, estejam influenciando os resultados.

### 4. Avaliação da performance do modelo

Após o ajuste do modelo com o conjunto de treinamento, realizamos previsões sobre o conjunto de teste e calculamos as métricas de avaliação usando `classification_report`. Abaixo estão os resultados típicos das métricas mais comuns:

  - **Acurácia:** Proporção de previsões corretas em relação ao total de previsões.
  - **Precisão:** A porcentagem de classificações positivas que realmente são positivas.
  - **Recall (Sensibilidade):** A capacidade do modelo de identificar corretamente todas as instâncias positivas.
  - **F1-Score:** A média harmônica entre precisão e recall, útil quando há um trade-off entre essas métricas.
"""

# Fazer previsões
y_pred = model.predict(X_test)

# Avaliar o desempenho
print(classification_report(y_test, y_pred))

"""Essas métricas oferecem uma visão geral da performance do modelo em cada uma das classes de acidentes.

---

### 5. Matriz de Confusão

A `matriz de confusão` é uma representação gráfica que compara as previsões do modelo com os valores reais. Cada célula da matriz representa a contagem de classificações corretas ou incorretas, facilitando a análise de onde o modelo está errando e onde está acertando.
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

y_pred = model.predict(X_test)  # Substitua por seu conjunto de teste
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predito')
plt.ylabel('Real')
plt.title('Matriz de Confusão')
plt.show()

"""- A diagonal principal representa as previsões corretas (verdadeiros positivos), enquanto as células fora da diagonal representam erros de classificação.
  - Podemos ver, por exemplo, quantos acidentes classificados como leves foram erroneamente classificados como graves e vice-versa.

### 6. Importância das Características

A importância das características ajuda a entender quais variáveis `(features)` influenciaram mais na decisão do modelo. Isso é especialmente útil para interpretar como cada fator contribui para a classificação dos acidentes.
"""

importances = model.feature_importances_
indices = np.argsort(importances)[::-1]
features = X.columns

plt.figure()
plt.title('Importância das Características')
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), features[indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.show()

"""  - O gráfico mostra a relevância de cada característica para o modelo. Por exemplo, podemos observar se a **causa do acidente** ou o **traçado da via** são as características mais importantes para determinar a gravidade dos acidentes.
  - Esse tipo de análise pode direcionar as autoridades de trânsito a focarem em políticas ou mudanças em infraestruturas associadas a fatores que mais impactam a ocorrência de acidentes graves
"""